<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="icon" type="image/png" sizes="32x32" href="./images/icon.png">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Public+Sans:wght@300;400;500;600&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="/dist/style.css">
    <title>CT super-resolution using Generative Adversarial Network</title>

</head>

<body>

    <div class="nav-back"></div>
    <header class="header">
        <div class="overlay has-fade"></div>
        <nav class="container container--pall flex flex-jc-sb flex-ai-c">
            <a href="/" class="header__logo">
                <img src="images/logo.svg" alt="Easybank">
            </a>

            <a id="btnHamburger" href="#" class="header__toggle hide-for-medium">
                <span></span>
                <span></span>
                <span></span>
            </a>

            <div class="header__links hide-for-xsmall">
                <a href="/">Projects</a>
                <a href="https://linkedin.com/in/gloria-ruxandra-maciuca-a60043135" target="_blank" rel="noopener noreferrer"><img src="/images/logo_linkedin.svg"></a>
                <a href="https://github.com/Gloria-M" target="_blank" rel="noopener noreferrer"><img
                        src="/images/logo_github.svg"></a>
            </div>
        </nav>

        <div class="header__menu has-fade">
            <a href="/">Projects</a>
            <a href="https://linkedin.com/in/gloria-ruxandra-maciuca-a60043135" target="_blank" rel="noopener noreferrer"><img src="/images/logo_linkedin.svg"></a>
            <a href="https://github.com/Gloria-M" target="_blank" rel="noopener noreferrer"><img
                    src="/images/logo_github.svg"></a>
        </div>

    </header>

    <div class="read">
        <div class="container container--pall">

            <div class="read__grid">

                <div class="toc-item hide-for-mobile">
                    <div class="toc__holder">
                        <ul class="toc__list-medium toc__list-medium__sr">
                            <li class="toc__sr">
                                <a href="#intro">
                                    <div class="toc__btn toc__btn__sr">
                                        Introduction
                                    </div>
                                </a>
                            </li>
                            <li class="toc__sr">
                            </li>
                            <li class="toc__sr">
                                <a href="#s1">
                                    <div class="toc__btn toc__btn__sr">
                                        Architecture
                                    </div>
                                </a>
                            </li>
                            <li class="toc__sr">
                                <a href="#s2">
                                    <div class="toc__btn-sub toc__btn-sub__sr">
                                        Generator
                                    </div>
                                </a>
                            </li>
                            <li class="toc__sr">
                                <a href="#s3">
                                    <div class="toc__btn-sub toc__btn-sub__sr">
                                        Feature Extractor
                                    </div>
                                </a>
                            </li>
                            <li class="toc__sr">
                                <a href="#s4">
                                    <div class="toc__btn-sub toc__btn-sub__sr">
                                        Discriminator
                                    </div>
                                </a>
                            </li>
                            <li class="toc__sr">
                                <a href="#s5">
                                    <div class="toc__btn-sub toc__btn-sub__sr">
                                        Generative Adversarial Network
                                    </div>
                                </a>
                            </li>
                            <li class="toc__sr">
                                <a href="#s6">
                                    <div class="toc__btn toc__btn__sr">
                                        Training Process
                                    </div>
                                </a>
                            </li>
                            <li class="toc__sr">
                                <a href="#s7">
                                    <div class="toc__btn toc__btn__sr">
                                        Evaluation
                                    </div>
                                </a>
                            </li>
                            <li class="toc__sr">
                                <a href="#s8">
                                    <div class="toc__btn toc__btn__sr">
                                        Experiment
                                    </div>
                                </a>
                            </li>
                        </ul>
                    </div>
                </div>

                <div class="content-item">
                    <div class="read__intro">
                        <div class="intro-item">
                            <div class="intro-item__title">
                                CT super-resolution using Generative Adversarial Network
                            </div>

                            <div class="read__sep-holder hide-for-desktop">
                                <div class="read__separator sr-bg"></div>
                            </div>

                            <div class="intro-item__to-code sr-text">
                                [
                                <a class="sr-text" href="https://github.com/Gloria-M/CT-super-resolution-mdrbGAN"
                                    target="_blank" rel="noopener noreferrer">Go to code</a>
                                ]
                            </div>
                        </div>
                    </div>

                    <div class="read__sep-holder hide-for-mobile">
                        <div class="read__separator sr-bg"></div>
                    </div>

                    <div class="read__content">


                        <div id="intro" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <p class="section__text">
                                This project presents the implementation of CT super-resolution using multiple dense residual block based GAN presented in 
                                <a class="section__cite srT-text" href="https://link.springer.com/article/10.1007/s11760-020-01790-5" target="_blank" rel="noopener noreferrer"> [Zhang et al., 2020]</a>, 
                                with slight changes as recommended in 
                                <a class="section__cite srT-text" href="https://arxiv.org/abs/1704.00028" target="_blank" rel="noopener noreferrer"> [Gulrajani et al., 2017]</a>
                                and with architecture parameters adapted from 
                                <a class="section__cite srT-text" href="https://arxiv.org/abs/1609.04802" target="_blank" rel="noopener noreferrer"> [Ledig et al., 2017]</a>, in case they were not specified.
                            </p>
                        </div>
                        

                        <div class="toc-item hide-for-desktop">
                            <div class="toc-mobile__holder">
                                <ul class="toc-mobile__list">
                                    <li class="toc-mobile__sr">
                                    </li>
                                    <li class="toc-mobile__sr">
                                        <a href="#s1">
                                            <div class="toc-mobile__btn toc-mobile__btn__sr">
                                                Architecture
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__sr">
                                        <a href="#s2">
                                            <div class="toc-mobile__btn-sub toc-mobile__btn-sub__sr">
                                                Generator
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__sr">
                                        <a href="#s3">
                                            <div class="toc-mobile__btn-sub toc-mobile__btn-sub__sr">
                                                Feature Extractor
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__sr">
                                        <a href="#s4">
                                            <div class="toc-mobile__btn-sub toc-mobile__btn-sub__sr">
                                                Discriminator
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__sr">
                                        <a href="#s5">
                                            <div class="toc-mobile__btn-sub toc-mobile__btn-sub__sr">
                                                Generative Adversarial Network
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__sr">
                                        <a href="#s6">
                                            <div class="toc-mobile__btn toc-mobile__btn__sr">
                                                Training Process
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__sr">
                                        <a href="#s7">
                                            <div class="toc-mobile__btn toc-mobile__btn__sr">
                                                Evaluation
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__sr">
                                        <a href="#s7">
                                            <div class="toc-mobile__btn toc-mobile__btn__sr">
                                                Experiment
                                            </div>
                                        </a>
                                    </li>
                                </ul>
                            </div>
                        </div>


                        <div id="s1" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__title">
                                Architecture
                            </div>
                            <div class="section__separator sr-bg"></div>
                            <p class="section__text">
                                The authors propose a generator structure with dense connections between residual blocks in order to reduce the network redundancy. In the computation of the loss function, they introduced the Wasserstein distance and a perceptual loss value extracted from feature maps obtained by a VGG-19 feature extractor.
                            </p>
                        </div>


                        <div id="s2" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__sstitle">
                                Generator
                            </div>
                            <div class="section__separator sr-bg"></div>
                            <p class="section__text">
                                The Generator recieves a low-resolution CT image as input and learns towards generating super-resolution CT image to match the high-resolution CT image.
                                As displayed in the image below, based on the author best results, the Generator architecture is represented a collection of <b>4</b> densely connected Residual Blocks, each of them consisting in <b>4</b> linked Residual Units, followed by Convolution and Batch normalization layers. Through <b>skip connections</b>, the low-resolution CT image is added to the output obtained from the first part of the generator and fed to another Convolution layer, followed by a flattening operation (for which I choose convolution with 1 kernel) and non-linearity induced by Parametric ReLU.
                            </p>
                            <div class="section__image-holder">
                                <img src="images\Generator_complete.png" width=100%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                   Architecture of the Generator model.
                                </p>
                            </div>
                            <p class="section__text">
                                Since some of the design parameters were not provided, I adapted this structure using kernels size as proposed in 
                                <a class="section__cite srT-text" href="https://arxiv.org/abs/1609.04802" target="_blank" rel="noopener noreferrer"> [Ledig et al., 2017]</a>.
                            </p>
                        </div>


                        <div id="s3" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__sstitle">
                                Feature Extractor
                            </div>
                            <div class="section__separator sr-bg"></div>
                            <p class="section__text">
                                The super-resolution CT images generated and the high-resolution CT real images are fed to a feature extractor, represented by a substructure of the <b>VGG-19</b> model
                                <a class="section__cite srT-text" href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener noreferrer"> [Simonyan & Zisserman, 2015]</a>.
                            </p>
                        </div>


                        <div id="s4" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__sstitle">
                                Discriminator
                            </div>
                            <div class="section__separator sr-bg"></div>
                            <p class="section__text">
                                Based mainly on the structure presented by the authors and with adaptations inspired again by 
                                <a class="section__cite srT-text" href="https://arxiv.org/abs/1609.04802" target="_blank" rel="noopener noreferrer"> [Ledig et al., 2017]</a>, the Discriminator architecture is composed of <b>8</b> Convolutional blocks with the same structure, excepting the first one. The number of convolution kernels is increased by a factor of 2, with alternating strided and not-strided operations, following the VGG network scheme. This part outputs <b>512</b> feature maps (representations of the super-resolution CT image or the high-resolution CT image), which are fed to a sequence of <b>2</b> linear layers with Leaky ReLU activation. The output of the discriminator represents the probability the input CT image is the high-resolution CT image.
                            </p>
                            <div class="section__image-holder">
                                <img src="images\DiscriminatorComplete.png" width=55%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                   Architecture of the Discriminator model.
                                </p>
                            </div>
                        </div>


                        <div id="s5" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__sstitle">
                                Generatve Adversarial Network
                            </div>
                            <div class="section__separator sr-bg"></div>
                            <p class="section__text">
                                The complete architecture of the Generative Adversarial Network based on multiple dense residual blocks is displayed below:
                            </p>
                            <div class="section__image-holder">
                                <img src="images\GAN.png" width=60%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                   Overall architecture of the proposed model.
                                </p>
                            </div>
                            <p class="section__text">
                                The Generator receives low-resolution CT images as input and generates super-resolution images. 
                            </p>
                            <p class="section__text">
                                The generated image, along with the real high-resolution CT image are fed to the VGG-19 Feature Extractor, resulting in feature maps, used to compute the <b>F-loss</b> involved in the optimization of the generator parameters.
                            </p>
                            <div class="section__image-holder">
                                <img src="images\ct_perceptual_loss.png" width=85%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                   F-loss formula.
                                </p>
                            </div>
                            <p class="section__text">
                                The discriminator is designed for use with <b>Wasserstein-loss</b> with gradient penalty, as described in 
                                <a class="section__cite srT-text" href="https://arxiv.org/abs/1704.00028" target="_blank" rel="noopener noreferrer"> [Gulrajani et al., 2017]</a>. Receiving as input low-resolution or high-resolution CT images, it outputs the probability score of the input being discriminated as high-resolution image.
                            </p>
                            <div class="section__image-holder">
                                <img src="images\ct_wasserstein_loss.png" width=85%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                   Wasserstein-loss formula.
                                </p>
                            </div>
                            <p class="section__text">
                                Based on the formulations described above, the <b>overall objective function</b>, can be defined as follows:
                            </p>
                            <div class="section__image-holder">
                                <img src="images\ct_overall_loss.png" width=85%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                   Overall Objective function.
                                </p>
                            </div>
                            <p class="section__text">
                                The generator and discriminator loss functions are modified in the following manner:
                            </p>
                            <div class="section__image-holder">
                                <img src="images\ct_discriminator_generator_loss.png" width=85%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                   Generator loss function <b><em>(top)</em></b> and the discriminator loss function <b><em>(bottom)</em></b>.
                                </p>
                            </div>
                        </div>                        


                        <div id="s6" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__title">
                                Training Process
                            </div>
                            <div class="section__separator sr-bg"></div>
                            <p class="section__text">
                                The training is carried out on <b>96&times;96</b> non-overlapping patches extracted from <b>512&times;512</b> CT images, in batches of 16. The pixels values are truncated and the images are scaled to interval <b>0 .. 1</b>. The low-resolution CT images are obtained by scaling down the high-resolution CT images by a factor of <b>4</b>, followed by upscaling to the original dimension using <b>bicubic interpolation</b>.
                            </p>
                            <div class="section__image-holder">
                                <img src="images\low_high_res_ct.png" width=85%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                   Training CT data: low-resolution image <b><em>(left)</em></b> and high-resolution image <b><em>(right)</em></b>.
                                </p>
                            </div>
                            <p class="section__text">
                                Different than the solution proposed by the authors, the optimization of the discriminator and the generator are performed by <b>Adam optimizer</b> with initial learning rate of <b>1e-4</b> and decay factor of <b>1e-1</b>.
                                Following the recomendation in 
                                <a class="section__cite srT-text" href="https://arxiv.org/abs/1704.00028" target="_blank" rel="noopener noreferrer"> [Gulrajani et al., 2017]</a>, the discriminator parameters are updated <b>5</b> iterations for each update of the generator parameters. 
                            </p>
                        </div>                     


                        <div id="s7" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__title">
                                Evaluation
                            </div>
                            <div class="section__separator sr-bg"></div>
                            <p class="section__text">
                                For the evaluation phase, the learned weights are applied to patches of size <b>128&times;128</b> obtained by dividing unseen CT slices, in order to be able to reconstruct the image, as illustrated below. The low-resolution CT images are obtained the same way as in the case of the train data.
                            </p>
                            <div class="section__image-holder">
                                <img src="images\test_patches.png" width=45%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                   Dividng the test CT slices in <b>128&times;128</b> patches.
                                </p>
                            </div>
                            <p class="section__text">
                            </p>
                        </div>                 


                        <div id="s8" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__title">
                                Experiment
                            </div>
                            <div class="section__separator sr-bg"></div>
                            <p class="section__text">
                                The experiment was carried out on a small subset of the <b>MosMed COVID-19 Dataset</b> 
                                <a class="section__cite srT-text" href="https://journals.eco-vector.com/DD/article/view/46826" target="_blank" rel="noopener noreferrer"> [Morozov et al., 2020]</a>. The dataset consist in multiple chest CT scans of healthy and infected patients, with different number of scan slices and slices thickness. 
                            </p>
                            <p class="section__text">
                                Restricted by the high computational resources needed to conduct an experiment as described by the authors, I selected <b>16</b> CT scans for training, <b>2</b> CT scans for validation and <b>6</b> CT scans for testing and used only <b>1</b> slice from each scan.
                            </p>
                            <p class="section__text">
                                Using the parameters described above, the model was trained and tested on the validation data for <b>1500</b> epochs.
                                I find the evolution of the generated images interesting to observe and gain insights on the convolution operations and patterns discovered during training.
                            </p>
                            <div class="section__image-holder">
                                <img src="images\CT_training_evolution.png" width=100%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                   Generator output at different steps in the training process.
                                </p>
                            </div>
                            <p class="section__text">
                                The experiment concludes with testing the trained GAN model on the unseen CT slices, with the results shown below:
                            </p>
                            <div class="section__image-holder">
                                <img src="images\test_results_205_1500.png" width=100%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text"></p>
                            </div>
                            <div class="section__image-holder">
                                <img src="images\test_results_215_1500.png" width=100%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text"></p>
                            </div>
                            <div class="section__image-holder">
                                <img src="images\test_results_225_1500.png" width=100%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text"></p>
                            </div>
                            <div class="section__image-holder">
                                <img src="images\test_results_235_1500.png" width=100%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text"></p>
                            </div>
                            <div class="section__image-holder">
                                <img src="images\test_results_245_1500.png" width=100%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text"></p>
                            </div>
                            <div class="section__image-holder">
                                <img src="images\test_results_255_1500.png" width=100%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                   Test results: low-resolution input image <b><em>(left)</em></b>, generated super-resolution image  <b><em>(center)</em></b> and real high-resolution image  <b><em>(right)</em></b>.
                                </p>
                            </div>
                        </div>





                    </div>


                    

                </div>
            </div>

        </div>
    </div>


    <script src="/app/js/script.js"></script>
</body>

</html>