<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="icon" type="image/png" sizes="32x32" href="./images/icon.png">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Public+Sans:wght@300;400;500;600&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="/dist/style.css">
    <title>New Dataset for multimodal Music Emotion Recognition</title>

</head>

<body>

    <div class="nav-back"></div>
    <header class="header">
        <div class="overlay has-fade"></div>
        <nav class="container container--pall flex flex-jc-sb flex-ai-c">
            <a href="/" class="header__logo">
                <img src="images/logo.svg" alt="Easybank">
            </a>

            <a id="btnHamburger" href="#" class="header__toggle hide-for-medium">
                <span></span>
                <span></span>
                <span></span>
            </a>

            <div class="header__links hide-for-xsmall">
                <a href="/">Projects</a>
                <a href="https://linkedin.com/in/gloria-ruxandra-maciuca-a60043135" target="_blank" rel="noopener noreferrer"><img src="/images/logo_linkedin.svg"></a>
                <a href="https://github.com/Gloria-M" target="_blank" rel="noopener noreferrer"><img
                        src="/images/logo_github.svg"></a>
            </div>
        </nav>

        <div class="header__menu has-fade">
            <a href="/">Projects</a>
            <a href="https://linkedin.com/in/gloria-ruxandra-maciuca-a60043135" target="_blank" rel="noopener noreferrer"><img src="/images/logo_linkedin.svg"></a>
            <a href="https://github.com/Gloria-M" target="_blank" rel="noopener noreferrer"><img
                    src="/images/logo_github.svg"></a>
        </div>

    </header>

    <div class="read">
        <div class="container container--pall">

            <div class="read__grid">

                <div class="toc-item hide-for-mobile">
                    <div class="toc__holder">
                        <ul class="toc__list-medium toc__list-medium__dataset">
                            <li class="toc__dataset">
                                <a href="#intro">
                                    <div class="toc__btn toc__btn__dataset">
                                        Introduction
                                    </div>
                                </a>
                            </li>
                            <li class="toc__dataset">
                                <a href="#s0">
                                    <div class="toc__btn toc__btn__dataset">
                                        How do people listen to music
                                    </div>
                                </a>
                            </li>
                            <li class="toc__dataset">
                                <a href="#s1">
                                    <div class="toc__btn toc__btn__dataset">
                                        Emotion Representation
                                    </div>
                                </a>
                            </li>
                            <li class="toc__dataset">
                                <a href="#s2">
                                    <div class="toc__btn toc__btn__dataset">
                                        Base Dataset
                                    </div>
                                </a>
                            </li>
                            <li class="toc__dataset">
                                <a href="#s3">
                                    <div class="toc__btn toc__btn__dataset">
                                        Lyrics&Comments Gathering
                                    </div>
                                </a>
                            </li>
                            <li class="toc__dataset">
                                <a href="#s4">
                                    <div class="toc__btn toc__btn__dataset">
                                        Annotation
                                    </div>
                                </a>
                            </li>
                            <li class="toc__dataset">
                                <a href="#s5">
                                    <div class="toc__btn toc__btn__dataset">
                                        Deezer Tracks Selection
                                    </div>
                                </a>
                            </li>
                            <li class="toc__dataset">
                                <a href="#s6">
                                    <div class="toc__btn toc__btn__dataset">
                                        Lyrics Cleaning
                                    </div>
                                </a>
                            </li>
                            <li class="toc__dataset">
                                <a href="#s7">
                                    <div class="toc__btn toc__btn__dataset">
                                        Lyrics&Comments Processing
                                    </div>
                                </a>
                            </li>
                        </ul>
                    </div>
                </div>

                <div class="content-item">
                    <div class="read__intro">
                        <div class="intro-item">
                            <div class="intro-item__title">
                                New Dataset for multimodal Music Emotion Recognition
                            </div>

                            <div class="read__sep-holder hide-for-desktop">
                                <div class="read__separator dataset-bg"></div>
                            </div>

                            <div class="intro-item__to-code dataset-text">
                                [
                                <a class="dataset-text" href="https://github.com/Gloria-M/multimodal-MER-dataset"
                                    target="_blank" rel="noopener noreferrer">Go to dataset</a>
                                ]
                            </div>
                        </div>
                    </div>

                    <div class="read__sep-holder hide-for-mobile">
                        <div class="read__separator dataset-bg"></div>
                    </div>


                    <div class="read__content">

                        <div id="intro" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <p class="section__text">
                                From the desire to model as accurate as possible the timeline described by humans when
                                listening to music, emerged the necessity of creating a new dataset, to provide support
                                for
                                recognizing emotions not only from audio features, but also from lyrics and comments
                                features.
                                The annotations are represented by scores in the two dimensions of emotion —valence and
                                arousal— and lists of emotion-related word & count tuples.
                            </p>
                        </div>

                        <div class="toc-item hide-for-desktop">
                            <div class="toc-mobile__holder">
                                <ul class="toc-mobile__list">
                                    <li class="toc-mobile__dataset">
                                    </li>
                                    <li class="toc-mobile__dataset">
                                        <a href="#s0">
                                            <div class="toc-mobile__btn toc-mobile__btn__dataset">
                                                How do people listen to music
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__dataset">
                                        <a href="#s1">
                                            <div class="toc-mobile__btn toc-mobile__btn__dataset">
                                                Emotion Representation
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__dataset">
                                        <a href="#s2">
                                            <div class="toc-mobile__btn toc-mobile__btn__dataset">
                                                Base Dataset
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__dataset">
                                        <a href="#s3">
                                            <div class="toc-mobile__btn toc-mobile__btn__dataset">
                                                Lyrics&Comments Gathering
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__dataset">
                                        <a href="#s4">
                                            <div class="toc-mobile__btn toc-mobile__btn__dataset">
                                                Annotation
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__dataset">
                                        <a href="#s5">
                                            <div class="toc-mobile__btn toc-mobile__btn__dataset">
                                                Deezer Tracks Selection
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__dataset">
                                        <a href="#s6">
                                            <div class="toc-mobile__btn toc-mobile__btn__dataset">
                                                Lyrics Cleaning
                                            </div>
                                        </a>
                                    </li>
                                    <li class="toc-mobile__dataset">
                                        <a href="#s7">
                                            <div class="toc-mobile__btn toc-mobile__btn__dataset">
                                                Lyrics&Comments Processing
                                            </div>
                                        </a>
                                    </li>
                                </ul>
                            </div>
                        </div>

                        <div id="s0" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__title">
                                How do people listen to music
                            </div>
                            <div class="section__separator dataset-bg"></div>
                            <p class="section__text">
                                Referring to the relation between the melody/audio dimension and the lyrics dimension,
                                the question "How do people listen to music?" seems to preoccupy individuals, as it was
                                answered on various forums and surveys:
                                <ul class="section__text">
                                    <li>
                                        "Do you pay more attention to the music or the lyrics?"
                                        <a class="section__cite dataset-text"
                                            href="https://v1.escapistmagazine.com/forums/read/18.404863-Poll-Do-you-pay-more-attention-to-the-music-or-the-lyrics"
                                            target="_blank" rel="noopener noreferrer">
                                            [Escapist]
                                        </a>
                                    </li>
                                    <li>
                                        "Do people generally pay attention to the lyrics in music?"
                                        <a class="section__cite dataset-text"
                                            href="https://www.quora.com/Do-people-generally-pay-attention-to-the-lyrics-in-music"
                                            target="_blank" rel="noopener noreferrer">
                                            [Quora]
                                        </a>
                                    </li>
                                    <li>
                                        "Do you often find meaning or inspiration in lyrics, or are you more interested
                                        in the music or beat?"
                                        <a class="section__cite dataset-text"
                                            href="https://learning.blogs.nytimes.com/2014/01/10/how-closely-do-you-listen-to-song-lyrics/"
                                            target="_blank" rel="noopener noreferrer">
                                            [NYTimes]
                                        </a>
                                    </li>
                                    <li>
                                        "Anyone else listen to music just for music?"
                                        <a class="section__cite dataset-text"
                                            href="https://www.reddit.com/r/Music/comments/eiq08/when_i_listen_to_music_i_dont_pay_attention_to/"
                                            target="_blank" rel="noopener noreferrer">
                                            [Reddit]
                                        </a>
                                    </li>
                                </ul>
                            </p>
                            <p class="section__text section__anchor-space">
                                Reading these opinions, patterns of the musical experience can be extracted and analysed. While the importance given to lyrics and solely audio may differ at first auditions, for most of the respondents listening and paying attention to the lyrics comes closely followed by trying to understand their meaning.
                                One comment that puts these observations in a beautiful and concluding form is <b><em>"enjoying the music will always be instinctual, while appreciating lyrics will always be intellectual"</em></b>.
                            </p>
                            <p class="section__text">
                                The summarization of the comments on this subject can also be shaped as a timeline, as shown below:
                            </p>
                            <div id="timeline" class="section__image-holder section__anchor-space">
                                <img src="images\HowPeopleListenToMusic.png" width=100%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                    People auditioning music with lyrics - Timeline (song: Fiona Apple - I know)<br>Lyrics and comment source:  <em>https://songmeanings.com/</em>.
                                </p>
                            </div>
                            <p class="section__text">
                                These ideas shape the musical experience consisting in two aspects: listening —audio dimension— and understanding the meaning, that implies paying attention to the lyrics text and analysing their meaning —text dimension. This motivates the need for builing a new dataset that allows for a more comprehensive analysis and understanding of the Music Emotion Recognition task.
                            </p>
                        </div>

                        <div id="s1" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__title">
                                Emotion Representation
                            </div>
                            <div class="section__separator dataset-bg"></div>
                            <p class="section__text">
                                The Circumplex Model of Affect, defined in
                                <a class="section__cite dataset-text"
                                    href="https://www.researchgate.net/publication/235361517_A_Circumplex_Model_of_Affect"
                                    target="_blank" rel="noopener noreferrer">
                                    [Russell, 1980]
                                </a>
                                , that maps emotions to a 2D space defined by <em>valence</em> and <em>arousal</em>
                                dimensions. Valence, displayed as the horizontal axis, represents the energy of emotion,
                                ranging from positive to negative, while arousal, the vertical axis, represents the
                                amount of intensity in emotion, ranging from low to high. This scheme allows for a
                                division of the emotion space in four quadrants.
                            </p>
                            <div class="section__image-holder">
                                <img src="images\circumplex_model.png" width=90%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                    Russell’s Circumplex Model of Affect - Dimensional representation of emotions.
                                </p>
                            </div>
                        </div>

                        <div id="s2" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__title">
                                Base Dataset
                            </div>
                            <div class="section__separator dataset-bg"></div>
                            <p class="section__text">
                                The largest dataset available that suits a regression approach of MER task is, at the
                                moment of writing, the Deezer Mood Detection Dataset
                                <a class="section__cite dataset-text" href="https://arxiv.org/abs/1809.07276"
                                    target="_blank" rel="noopener noreferrer">
                                    [Delbouys et al., 2018]
                                </a>
                                . For each track, this set contains the corresponding <b>Deezer ID</b>, the <b>MSD
                                    ID</b>, the <b>artist</b> and <b>title</b> and values for <b>valence</b> and
                                <b>arousal</b>. This data collected by researchers at Deezer is itself derived from the
                                Million Song Dataset
                                <a class="section__cite dataset-text"
                                    href="https://www.researchgate.net/publication/220723656_The_Million_Song_Dataset"
                                    target="_blank" rel="noopener noreferrer">
                                    [Bertin-Mahieux et al., 2011]
                                </a>
                                , a common support for MIR tasks in general.
                            </p>
                        </div>

                        <div id="s3" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__title">
                                Lyrics and Comments Gathering
                            </div>
                            <div class="section__separator dataset-bg"></div>
                            <p class="section__text">
                                For building the new dataset, the first step is selecting the songs for which lyrics and
                                comments from users are found when scraping the web. The
                                <a class="section__cite dataset-text" href="https://songmeanings.com/" target="_blank"
                                    rel="noopener noreferrer">
                                    SongMeanings
                                </a>
                                site provides both the lyrics of the songs and comments on the meaning of the lyrics, as
                                they are interpreted by the users.
                            </p>
                            <p class="section__text">
                                On the page of each track, the corresponding lyrics are identified by the <code
                                    class="text-dark">div class_='holder lyric-box'</code> tag and the comments section
                                is announced by the <code class="text-dark">div class_='text'</code> tag.

                                Acquiring the comments from this website comes with an important advantage that diminish
                                the need for additional filtering: the comments are sorted based on the number of votes
                                in descending order with the most voted comment -the most reliable one- listed first.
                                The comments are displayed on multiple pages, but the first one contains just enough
                                comments of good quality, therefore only the data in the first page is collected.

                                Starting with the 18,644 tracks selected by the researchers at Deezer, after performing
                                this procedure and removing the tracks that do not appear on the website or do not have
                                comments associated, the dataset size is reduced to 10,923 samples.
                            </p>
                        </div>

                        <div id="s4" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__title">
                                Annotation
                            </div>
                            <div class="section__separator dataset-bg"></div>
                            <p class="section__text">
                                The annotation process of the new dataset is performed by following steps similar to
                                <a class="section__cite dataset-text"
                                    href="https://www.semanticscholar.org/paper/Lyric-Text-Mining-in-Music-Mood-Classification-Hu-Downie/e658ec86e033aae370ba680118a04431071cafe1"
                                    target="_blank" rel="noopener noreferrer">
                                    [Hu et al.,2009]
                                </a>
                                , making use of the social tags associated with songs by listeners.
                                Mapping the tracks in the Deezer dataset to the <b>last.fm</b> database by their MSD ID,
                                the social tags recorded in year 2011 can be retrieved. Along each tag associated with
                                each song, a normalized value representing the number of listeners choosing the tag is
                                provided.
                            </p>
                            <p class="section__text">
                                In order to create the new dataset with information as up-to-date as possible, instead
                                of querying the 2011 tags database, the tags available at the time of writing are
                                acquired using
                                <a class="section__cite dataset-text" href="https://www.last.fm/api/" target="_blank"
                                    rel="noopener noreferrer">
                                    Last.fm API
                                </a>
                                . This allows access to a maximum of 100 tags per song, sorted by their popularity and
                                the normalized value indicating the count of listeners attributing the respective tag to
                                a song. Using this method, 10,919 tracks with tags are found.
                            </p>
                            <p class="section__text">
                                The considerable volume of tags collected (795,368 unique tags) is extremely noisy and
                                contains lot of unusable data. Therefore, to get to a format which allows mapping to
                                emotions space, a thorough cleaning process must be performed. The musical genre,
                                instruments names or similarity tags, along many others, are removed following the
                                removal criteria presented, reducing the volume of social tags to 390,698:
                                <ul class="section__text list-space">
                                    <li>tags containing artist name</li>
                                    <li>tags containing song title</li>
                                    <li>tags with similar recommendation</li>
                                    <li>numeric tags</li>
                                    <li>tags containing forms of word <em>favorite</em></li>
                                    <li>tags with musical genre reference</li>
                                    <li>tags with musical instruments reference</li>
                                    <li>tags containing nationalities words</li>
                                </ul>
                            </p>
                            <p class="section__text">
                                In order to select the tags akin to an emotion descriptor, a list of affect-related
                                words must first be composed. For this process, the affective lexicon WordNet Affect
                                <a class="section__cite dataset-text"
                                    href="http://corpustext.com/reference/affect_wordnet.html" target="_blank"
                                    rel="noopener noreferrer">
                                    [Valitutti, 2004]
                                </a>
                                is used to create the base. This list is cleaned and extended with synonyms, related
                                words and keywords contained in expressions describing motions, found using the
                                comprehensive Merriam-Webster online
                                <a class="section__cite dataset-text"
                                    href="https://www.merriam-webster.com/dictionary/dictionary" target="_blank"
                                    rel="noopener noreferrer">
                                    dictionary
                                </a>
                                and
                                <a class="section__cite dataset-text" href="https://www.merriam-webster.com/thesaurus"
                                    target="_blank" rel="noopener noreferrer">
                                    thesaurus
                                </a>
                                . These words are grouped according to their meaning in 27 emotional categories, named
                                with a corresponding emotion, using similarity paths defined in WordNet
                                <a class="section__cite dataset-text"
                                    href="https://www.researchgate.net/publication/228902778_Revising_the_WORDNET_DOMAINS_Hierarchy_semantics_coverage_and_balancing"
                                    target="_blank" rel="noopener noreferrer">
                                    [Bentivogli et al., 2004]
                                </a>
                                and, again, the Merriam-Webster Thesaurus.
                            </p>
                            <div class="section__image-holder">
                                <img src="images\emotions_groups_bg.png" width=90%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                    Grouping emotions extracted from tags.
                                </p>
                            </div>
                            <p class="section__text">
                                Before going further, words related to emotions, but which, in the context of social
                                tags, could have ambiguous or judgmental meaning are removed from the list, as suggested
                                in
                                <a class="section__cite dataset-text"
                                    href="https://www.semanticscholar.org/paper/Lyric-Text-Mining-in-Music-Mood-Classification-Hu-Downie/e658ec86e033aae370ba680118a04431071cafe1"
                                    target="_blank" rel="noopener noreferrer">
                                    [Hu et al.,2009]
                                </a>
                                . Therefore, appreciation remarks such as <em>'awesome', 'bad', 'fantastic', 'good',
                                    'great', 'horrible'</em> which refer to the quality of music and preference words
                                like <em>'adore', 'like', 'love', 'hate'</em> which might lead to ambiguous
                                interpretations, are removed from the list of emotion-related words.
                            </p>
                            <p class="section__text">
                                In order to include all possible forms of the emotion-related words, they are stemmed to
                                a root using the Porter Stemmer functionality provided in the
                                <a class="section__cite dataset-text" href="https://www.nltk.org/book/" target="_blank"
                                    rel="noopener noreferrer">
                                    Natural Language Toolkit
                                </a>
                                . By searching tags containing these stems, they are also grouped, according to the
                                stems' membership in the emotions clusters, discarding the tags not containing any of
                                the stems of the emotion-related words. Following, a manual selection of tags is
                                performed, as not all tags containing stems of affective words describe an emotion
                                perceived when listening to a particular song. Tags that could, at the same time, be
                                mapped to a positive emotion and to a negative emotion are identified and removed. Some
                                of these tags with ambiguous meaning found are <em>'to listen when sad', 'to listen when
                                    happy', 'music for sad days', music for when you feel down'</em>.
                            </p>
                            <p class="section__text">
                                After this curation phase, each song has its remained tags replaced by the emotion words
                                describing the clusters the tags are part of. It is important to mention that the
                                normalized values associated with each tag at song-level, are taking into account when
                                mapping the tags to valence and arousal values. At this stage, after removing the songs
                                without at least one tag with affective information, the dataset size is reduced to
                                10,579 samples. Therefore, for each song, the tags are transformed into the words
                                describing the emotions clusters, keeping their corresponding normalized count. In the
                                case when more tags are mapped to the same word, the normalized count of the
                                representative word is obtained by summing the values of the tags.
                            </p>
                            <div class="section__image-holder">
                                <img src="images\tags_to_emotions_bg.png" width=100%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                    The process of converting social tags to emotion-related words. <br>Tags acquired from Last.fm(<em>https://www.last.fm/home</em>) - Song: Fiona Apple, I know.
                                </p>
                            </div>
                            <p class="section__text">
                                The
                                <a class="section__cite dataset-text"
                                    href="https://www.researchgate.net/publication/334118031_Obtaining_Reliable_Human_Ratings_of_Valence_Arousal_and_Dominance_for_20000_English_Words"
                                    target="_blank" rel="noopener noreferrer">
                                    NRC VAD Lexicon
                                </a>
                                comprises more than 20,000 English words with associated ratings of valence, arousal and
                                dominance given by human annotators. The annotators were presented with groups of 4
                                words from which, besides assigning real values of emotion dimensions, they had to
                                choose two words following a Best-Worst Scaling technique. The final scores for each
                                word were obtained by using the real values along with the proportions the respective
                                word was ranked as best or worst and scaling them to range 0 .. 1.
                            </p>
                            <p class="section__text">
                                Using this resource, the 27 words representing emotions are embedded into
                                valence-arousal space, taking into account their summed count at song-level.
                                For every song, the valence and arousal scores extracted from the VAD Lexicon for each
                                word are multiplied by the normalized count and the final ratings for the song are
                                obtained by computing the average of the sum of the weighted values for valence and
                                arousal dimensions:
                            </p>
                            <div class="section__image-holder">
                                <img src="https://render.githubusercontent.com/render/math?math=Valence(S) = \displaystyle{\frac{\sum_{w} valence_{w}\cdot count_{S,w}}{\sum_{w} count_{S,w}}}"
                                    width=45%>
                            </div>
                            <br>
                            <div class="section__image-holder">
                                <img src="https://render.githubusercontent.com/render/math?math=Arousal(S) = \displaystyle{\frac{\sum_{w} arousal_{w}\cdot count_{S,w}}{\sum_{w} count_{S,w}}}"
                                    width=45%>
                            </div>
                            <p class="section__text">
                                where <em>S</em> is a song in the dataset and <em>w</em> an emotion word in <em>S</em>.
                            </p>
                            <p id="distr-data" class="section__text section__anchor-space">
                                This annotation strategy resulted in an unbalanced distribution of the data in the 2D
                                space of emotions, with quadrant 2 appearing heavily underrepresented. This reveals that
                                strong negative emotions such as <em>'anger', 'aggressivity'</em> are not frequently
                                perceived in music.
                            </p>
                            <div class="section__image-holder">
                                <img src="images\data_distribution.png" width=50%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                    Distribution of data from the new dataset in the four quadrants of the 2D space of emotions.
                                </p>
                            </div>
                        </div>

                        <div id="s5" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__title">
                                Deezer Tracks Selection
                            </div>
                            <div class="section__separator dataset-bg"></div>
                            <p class="section__text">
                                The next phase in creating a new dataset with valid information, is updating the Deezer
                                IDs for the 10,579 annotated tracks. A number of 607 tracks are not found on Deezer
                                platform and for 1,441 tracks with outdated IDs, replacements are found, with identical
                                artist name and song title.
                            </p>
                            <p class="section__text">
                                Another filtering process is performed considering the duration of songs and 32 more
                                tracks are removed, bringing the dataset to a size of 9,972 samples.
                            </p>
                        </div>

                        <div id="s6" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__title">
                                Lyrics Cleaning
                            </div>
                            <div class="section__separator dataset-bg"></div>
                            <p class="section__text">
                                The raw lyrics data contain a great amount of noise, as they are written by different
                                users on <em>https://songmeanings.com/</em>, with no set of rules or format to be
                                followed. Therefore, cleaning the lyrics data is a meticulous process that involves
                                manual curating and a considerable amount of time. The lyrics have a particular
                                structure, different than usual text data. Hence, before applying cleaning procedures
                                commonly used in <em>NLP</em> tasks, this structure is studied for patterns
                                identification
                                with the scope of finding a set of rule that allow for automatization.
                            </p>
                            <p class="section__text">
                                In the raw lyrics text, there are different types of tags, such as repetitive
                                instructions (<em>'repeat', 'x2', 'repeat once'</em>, etc.), announcement of artists
                                singing a particular part of the song (<em>'[eminem]', '[all:]', '2pac'</em>, etc.),
                                structural information (<em>'chorus', 'verse', 'bridge', 'pre-chorus'</em>, etc.).
                                As a result of a series of manual transformations and removals, different types of
                                patterns are revealed and used for automatization of the cleaning process.
                                In differentiating between the types of patterns discovered, the <em>newline</em> (\n)
                                character has the greatest role.
                                The most important patterns are ones playing a role in changing the text of lyrics, as
                                described below:
                            </p>
                            <div class="section__image-holder">
                                <img src="images\lyrics_repetition_patterns.png" width=90%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                    Identified and created patterns leading to modifications in lyrics text <b>\n</b> - newline character, <b>N</b> - number of repetitions.
                                </p>
                            </div>
                            <p class="section__text">
                                The structural instructions are another class of patterns discovered, with no relevance
                                for the strategy chosen in recognizing emotions in music, therefore are removed from the
                                lyrics text. They may contain tags present in the important patterns mentioned earlier,
                                but in different configurations, as defined by the occurrence of the <em>newline</em>
                                character.
                            </p>
                            <div class="section__image-holder">
                                <img src="images\lyrics_structural_patterns.png" width=90%>
                            </div>
                            <div class="section__fig-caption">
                                <p class="section__fig-caption__text">
                                    Structural patterns in lyrics text.
                                </p>
                            </div>
                            <p class="section__text">
                                These transformations bring the lyrics text to a format that allows applying common
                                preprocessing techniques used in NLP tasks.
                            </p>
                        </div>

                        <div id="s7" class="section__anchor-space"></div>
                        <div class="section__holder">
                            <div class="section__title">
                                Lyrics and Comments preprocessing
                            </div>
                            <div class="section__separator dataset-bg"></div>
                            <p class="section__text">
                                Starting with the texts in <em>string</em> format, the following transformations are
                                applied:
                                <ul class="section__text list-space">
                                    <li>
                                        replace words in short form with their extended forms <br>
                                        - 'm &#8594 am <br>
                                        - 're &#8594 are <br>
                                        - 've &#8594 have <br>
                                        - 'd &#8594 would <br>
                                        - 'll &#8594 will <br>
                                        - she's &#8594 she is <br>
                                        - he's &#8594 he is <br>
                                        - it's &#8594 it is <br>
                                        - ain't &#8594 is not <br>
                                        - n't &#8594 n not <br>
                                        - n' &#8594 ng <br>
                                        - 's &#8594 ''
                                    </li>
                                    <li>
                                        remove punctuation symbols (<em>' . ' , ' , ' , ' ; ' , '!', '', '*'</em>, etc.)
                                    </li>
                                    <li>
                                        remove spacing instructions (<em>'\n', '\t'</em>)
                                    </li>
                                </ul>
                            </p>
                            <p class="section__text">
                                The steps presented above conclude with tokenization, converting the texts into lists of
                                words. The preprocessing phase ends after curating the words resulted, as follows:
                                <ul class="section__text list-space">
                                    <li>
                                        correct identified misspellings (*'urself', 'thered'*, etc.)
                                    </li>
                                    <li>
                                        replace censored words with their uncensored version
                                    </li>
                                    <li>
                                        remove stopwords
                                    </li>
                                    <li>
                                        discard non-english words, using
                                        <a class="section__cite dataset-text"
                                            href="http://pyenchant.github.io/pyenchant/" target="_blank"
                                            rel="noopener noreferrer">
                                            pyenchant
                                        </a>
                                    </li>
                                    <li>
                                        keep the non-english words that appear in the slangs lexicon
                                        <a class="section__cite dataset-text" href="https://arxiv.org/abs/1608.05129"
                                            target="_blank" rel="noopener noreferrer">
                                            [Wu et al., 2016]
                                        </a>
                                    </li>
                                </ul>
                            </p>
                        </div>




                    </div>

                </div>
            </div>

        </div>
    </div>


    <script src="/app/js/script.js"></script>
</body>

</html>